import seqio

import teva.tasks

include 't5x/configs/runs/pretrain.gin'

MIXTURE_OR_TASK_NAME = "news_and_wiki"
TASK_FEATURE_LENGTHS = {"inputs": 512, "targets": 114}  # TODO: @theyorubayesian
TRAIN_STEPS = 10000
DROPOUT_RATE = 0.0
BATCH_SIZE = 512

utils.RestoreCheckpointConfig:
  path = []  # initialize from scratch

utils.SaveCheckpointConfig:
  period = 50000
  dtype = 'float32'
  keep = None  # keep all checkpoints
  save_dataset = False  # don't checkpoint dataset state

trainer.Trainer:
  num_microbatches = 8
  learning_rate_fn = @utils.create_learning_rate_scheduler()

utils.create_learning_rate_scheduler:
  factors = 'constant * rsqrt_decay'
  base_learning_rate = 1.0
  warmup_steps = 10000  # 10k to keep consistent with T5/MTF defaults.