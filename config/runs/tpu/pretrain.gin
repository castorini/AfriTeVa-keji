import seqio

import teva.tasks

include 't5x/examples/t5/t5_1_1/base.gin'
VOCABULARY = @seqio.SentencePieceVocabulary()
seqio.SentencePieceVocabulary.sentencepiece_model_file = "gs://afriqa-bucket/tokenizers/sentencepiece.bpe.model"

models.EncoderDecoderModel:
    input_vocabulary = %VOCABULARY
    output_vocabulary = %VOCABULARY

include 't5x/configs/runs/pretrain.gin'

MIXTURE_OR_TASK_NAME = "news_and_wiki"
TASK_FEATURE_LENGTHS = {"inputs": 512, "targets": 114}  # TODO: @theyorubayesian
TRAIN_STEPS = 10000
DROPOUT_RATE = 0.0
BATCH_SIZE = 256

utils.RestoreCheckpointConfig:
  path = []  # initialize from scratch

utils.SaveCheckpointConfig:
  period = 1000
  dtype = 'float32'
  keep = None  # keep all checkpoints
  save_dataset = False  # don't checkpoint dataset state

trainer.Trainer:
  num_microbatches = None
  learning_rate_fn = @utils.create_learning_rate_scheduler()

utils.create_learning_rate_scheduler:
  factors = 'constant * rsqrt_decay'
  base_learning_rate = 1.0
  warmup_steps = 10000  # 10k to keep consistent with T5/MTF defaults.